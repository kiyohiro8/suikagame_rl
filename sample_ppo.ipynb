{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from env import SuikaEnv\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_envs = 1\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "# PPO Parameter\n",
    "learning_rate = 3e-4\n",
    "n_rollout_steps = 1024\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "clip_range = 0.2\n",
    "normalize_advantage = True\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "buffer_size = n_envs * n_rollout_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to COM3\n"
     ]
    }
   ],
   "source": [
    "env = SuikaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_frame_numbers = []\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBufferSamples(NamedTuple):\n",
    "    observations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    old_values: torch.Tensor\n",
    "    old_log_prob: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, buffer_size, n_envs, obs_shape, action_dim, device):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_envs = n_envs\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "    def reset(self):\n",
    "        self.observations = torch.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=torch.float32)\n",
    "        self.actions = torch.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=torch.int64)\n",
    "        self.rewards = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.returns = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.episode_starts = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.values = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.log_probs = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.advantages = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.pos = 0\n",
    "        self.generator_ready = False\n",
    "\n",
    "    def add(self, obs, action, reward, episode_start, value, log_prob):\n",
    "        self.observations[self.pos] = obs\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.episode_starts[self.pos] = episode_start\n",
    "        self.values[self.pos] = value.cpu().flatten()\n",
    "        self.log_probs[self.pos] = log_prob.cpu()\n",
    "        self.pos += 1\n",
    "\n",
    "    def compute_returns_and_advantage(self, last_values, dones):\n",
    "        last_values = last_values.cpu().flatten()\n",
    "\n",
    "        last_gae_lam = 0\n",
    "        for step in reversed(range(self.buffer_size)):\n",
    "            if step == self.buffer_size - 1:\n",
    "                next_non_terminal = 1.0 - dones.to(torch.float32)\n",
    "                next_values = last_values\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.episode_starts[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            delta = self.rewards[step] + gamma * next_values * next_non_terminal - self.values[step]\n",
    "            last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            self.advantages[step] = last_gae_lam\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    @staticmethod\n",
    "    def swap_and_flatten(arr):\n",
    "        shape = arr.shape\n",
    "        if len(shape) < 3:\n",
    "            shape = (*shape, 1)\n",
    "        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        indices = np.random.permutation(self.buffer_size * self.n_envs)\n",
    "\n",
    "        if not self.generator_ready:\n",
    "            self.observations = self.swap_and_flatten(self.observations)\n",
    "            self.actions = self.swap_and_flatten(self.actions)\n",
    "            self.values = self.swap_and_flatten(self.values)\n",
    "            self.log_probs = self.swap_and_flatten(self.log_probs)\n",
    "            self.advantages = self.swap_and_flatten(self.advantages)\n",
    "            self.returns = self.swap_and_flatten(self.returns)\n",
    "            self.generator_ready = True\n",
    "\n",
    "        start_idx = 0\n",
    "        while start_idx < self.buffer_size * self.n_envs:\n",
    "            yield self._get_samples(indices[start_idx : start_idx + batch_size])\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def to_torch(self, array):\n",
    "        return torch.as_tensor(array, device=self.device)\n",
    "\n",
    "    def _get_samples(\n",
    "        self,\n",
    "        batch_inds\n",
    "    ):\n",
    "        data = (\n",
    "            self.observations[batch_inds],\n",
    "            self.actions[batch_inds],\n",
    "            self.values[batch_inds].flatten(),\n",
    "            self.log_probs[batch_inds].flatten(),\n",
    "            self.advantages[batch_inds].flatten(),\n",
    "            self.returns[batch_inds].flatten(),\n",
    "        )\n",
    "        return RolloutBufferSamples(*tuple(map(self.to_torch, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "        self.fc_p = nn.Linear(512, 4)\n",
    "        self.fc_v = nn.Linear(512, 1)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc(x.flatten(1)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        policy = F.relu(self.fc_p(x))\n",
    "        value = F.relu(self.fc_v(x))\n",
    "        return policy, value\n",
    "    \n",
    "    def predict_values(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        value = F.relu(self.fc_v(x))\n",
    "        return value\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_prob(value, logits):\n",
    "        value, log_pmf = torch.broadcast_tensors(value, logits)\n",
    "        value = value[..., :1]\n",
    "        log_prob = log_pmf.gather(-1, value).squeeze(-1)\n",
    "        return log_prob\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(logits):\n",
    "        min_real = torch.finfo(logits.dtype).min\n",
    "        logits = torch.clamp(logits, min=min_real)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        p_log_p = logits * probs\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        logits, values = self.forward(obs)\n",
    "        # Normalize\n",
    "        logits = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        actions = torch.multinomial(probs, 1, True)\n",
    "        return actions, values, self.log_prob(actions, logits)\n",
    "    \n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        # Normalize\n",
    "        logits = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
    "        log_prob = self.log_prob(actions, logits)\n",
    "        entropy = self.entropy(logits)\n",
    "        return values, log_prob, entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "switch_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
